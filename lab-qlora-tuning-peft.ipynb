{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s11khushboo/lab-qlora-tuning-peft/blob/main/lab-qlora-tuning-peft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Y40X0WCY_H"
      },
      "source": [
        "# Lab | QLoRA Tuning using PEFT from Hugging Face\n",
        "\n",
        "<!-- ### Introduction to Quantization & Fine-tune a Quantized Model -->\n",
        "\n",
        "**Note:** This is more or less the same notebook you saw in the previous lesson, but that is ok. This is an LLM fine-tuning lab. In class we used a set of datasets and models, and in the labs you are required to change the LLMs models and the datasets including the pre-processing pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_afIANF7iKXp"
      },
      "source": [
        "# Brief Introduction to Quantization\n",
        "The main idea of quantization is simple: Reduce the precision of floating-point numbers, which normally occupy 32 bits, to integers of 8 or even 4 bits.\n",
        "\n",
        "This reduction occurs in the model’s parameters, specifically in the weights of the neural layers, and in the activation values that flow through the model’s layers.\n",
        "\n",
        "This means that we not only achieve an improvement in the model’s storage size and memory consumption but also greater agility in its calculations.\n",
        "\n",
        "Naturally, there is a loss of precision, but particularly in the case of 8-bit quantization, this loss is minimal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyoaSeEAi8_W"
      },
      "source": [
        "## Let's see a example of a quantized number.\n",
        "\n",
        "In reality, what I want to examine is the precision loss that occurs when transitioning from a 32-bit number to a quantized 8/4-bit number and then returning to its original 32-bit value.\n",
        "\n",
        "First, I'm going to create a function to quantize and another to unquantize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yj5-xG8WogNP"
      },
      "outputs": [],
      "source": [
        "#Importing necesary linbraries\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k17vVVU9iKws"
      },
      "outputs": [],
      "source": [
        "#Functions to quantize and unquantize\n",
        "def quantize(value, bits=4):\n",
        "    quantized_value = np.round(value * (2**(bits - 1) - 1))\n",
        "    return int(quantized_value)\n",
        "\n",
        "def unquantize(quantized_value, bits=4):\n",
        "    value = quantized_value / (2**(bits - 1) - 1)\n",
        "    return float(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vtGC-Mhh3nH"
      },
      "source": [
        "Quatizied values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXRJ7mJMlpjA",
        "outputId": "04baf689-b525-4d5d-e7c8-7f50dbff73f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "79\n"
          ]
        }
      ],
      "source": [
        "quant_4 = quantize(0.622, 4)\n",
        "print (quant_4)\n",
        "quant_8 = quantize(0.622, 8)\n",
        "print(quant_8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN7K4714h8S8"
      },
      "source": [
        "Unquantized values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y50sGnmbmCqv",
        "outputId": "85024f72-2e22-467b-beb7-4c7844824595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5714285714285714\n",
            "0.6220472440944882\n"
          ]
        }
      ],
      "source": [
        "unquant_4 = unquantize(quant_4, 4)\n",
        "print(unquant_4)\n",
        "unquant_8 = unquantize(quant_8, 8)\n",
        "print(unquant_8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyjuQtfFTalb"
      },
      "source": [
        "If we consider that the original number was 0.622, it can be said that 8-bit quantization barely loses precision, and the loss from 4-bit quantization is manageable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KzCAXBmMnNSA"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-1, 1, 50)\n",
        "y = [math.cos(val) for val in x]\n",
        "\n",
        "\n",
        "y_quant_8bit = np.array([quantize(val, bits=8) for val in y])\n",
        "y_unquant_8bit = np.array([unquantize(val, bits=8) for val in y_quant_8bit])\n",
        "\n",
        "y_quant_4bit = np.array([quantize(val, bits=4) for val in y])\n",
        "y_unquant_4bit = np.array([unquantize(val, bits=4) for val in y_quant_4bit])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKYNb0nMjWWu"
      },
      "source": [
        "Let’s plot a curve with the unquantized values of a cosine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "u35LgstBoaTQ",
        "outputId": "f9b2b8da-3cff-4d8d-e885-738d3898401d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 12))\n",
        "\n",
        "plt.subplot(4, 1, 1)\n",
        "plt.plot(x, y, label=\"Original\")\n",
        "plt.plot(x, y_unquant_8bit, label=\"unquantized_8bit\")\n",
        "plt.plot(x, y_unquant_4bit, label=\"unquantized_4bit\")\n",
        "plt.legend()\n",
        "plt.title(\"Quantized Curves Graph Comparision\")\n",
        "plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLv-uYtlT2Bo"
      },
      "source": [
        "As you can see, the difference between the 8-bit and the original values is minimal. However, we need to use 4-bit quantization if we want to load the 7B Model into a 16GB GPU without problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "620MdVMk7iUS"
      },
      "source": [
        "\n",
        "# QLoRA. Fine-tuning a 4-bit Quantized Model using LoRA.\n",
        "We are going to fine-tune with LoRA a 7B Model Quantizated to 4 bits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uml3wgdri2_X"
      },
      "source": [
        "## Load the PEFT and Datasets Libraries.\n",
        "\n",
        "The PEFT library contains the Hugging Face implementation of differente fine-tuning techniques, like LoRA Tuning.\n",
        "\n",
        "Using the Datasets library we have acces to a huge amount of Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_UyyuMGnCPjA"
      },
      "outputs": [],
      "source": [
        "# !pip install -q accelerate==0.29.3\n",
        "# !pip install -q bitsandbytes==0.43.1\n",
        "# !pip install -q trl==0.8.6\n",
        "# !pip install -q peft==0.10.0\n",
        "# !pip install -q transformers==4.40.0\n",
        "# !pip install -q triton==2.2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y bitsandbytes triton torch transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiGdlUtB_glv",
        "outputId": "5979ca77-3a51-4a06-b0a0-8b19ebac4c57"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: triton 3.4.0\n",
            "Uninstalling triton-3.4.0:\n",
            "  Successfully uninstalled triton-3.4.0\n",
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "Found existing installation: transformers 4.57.1\n",
            "Uninstalling transformers-4.57.1:\n",
            "  Successfully uninstalled transformers-4.57.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.3.0+cu121 torchvision==0.18.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install bitsandbytes==0.42.0\n",
        "!pip install transformers==4.44.2 peft==0.11.1 accelerate==0.33.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y5AAP2Y_GUFc",
        "outputId": "bdf64d52-0eb7-4f1d-b774-ce133cfebfd4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.3.0+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp312-cp312-linux_x86_64.whl (780.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.18.0+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp312-cp312-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0+cu121) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0+cu121) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0+cu121) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0+cu121) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0+cu121) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0+cu121) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m136.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.0+cu121) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.18.0+cu121) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0+cu121) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.0+cu121) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.0+cu121) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "peft 0.17.1 requires transformers, which is not installed.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.3.0+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.0+cu121 torchvision-0.18.0+cu121\n",
            "Collecting bitsandbytes==0.42.0\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.42.0) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->bitsandbytes==0.42.0) (2.0.2)\n",
            "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.42.0\n",
            "Collecting transformers==4.44.2\n",
            "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.11.1\n",
            "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting accelerate==0.33.0\n",
            "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (0.6.2)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.2)\n",
            "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (2.3.0+cu121)\n",
            "Collecting numpy>=1.17 (from transformers==4.44.2)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2) (1.2.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.11.1) (12.6.85)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.11.1) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.13.0->peft==0.11.1) (1.3.0)\n",
            "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m127.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tokenizers, transformers, accelerate, peft\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.11.0\n",
            "    Uninstalling accelerate-1.11.0:\n",
            "      Successfully uninstalled accelerate-1.11.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.17.1\n",
            "    Uninstalling peft-0.17.1:\n",
            "      Successfully uninstalled peft-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.33.0 numpy-1.26.4 peft-0.11.1 tokenizers-0.19.1 transformers-4.44.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "134b0d82e1c540d0805e4b4ee9c73f3f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuYNBSmTbvtB"
      },
      "source": [
        "I'm going to download the peft and Transformers libraries from their repositories on GitHub instead of using pip. This is not strictly necessary, but this way, you can get the newest versions of the libraries with support for newer models. If you want to check one of the latest models, you can use this trick.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VVe5LcY9deQ5"
      },
      "outputs": [],
      "source": [
        "#Install the lastest versions of peft & transformers library recommended\n",
        "#if you want to work with the most recent models\n",
        "#!pip install -q git+https://github.com/huggingface/peft.git\n",
        "#!pip install -q git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U trl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hDuiLd-qI1qM",
        "outputId": "095069b7-5430-4c71-b69b-f3eb195daad5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl\n",
            "  Downloading trl-0.25.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting accelerate>=1.4.0 (from trl)\n",
            "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Collecting transformers>=4.56.1 (from trl)\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2024.11.6)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.56.1->trl)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.10.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
            "Downloading trl-0.25.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m139.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, accelerate, trl\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.33.0\n",
            "    Uninstalling accelerate-0.33.0:\n",
            "      Successfully uninstalled accelerate-0.33.0\n",
            "Successfully installed accelerate-1.11.0 tokenizers-0.22.1 transformers-4.57.1 trl-0.25.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "79ee440835514481822eb2a6254c0b00"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOnJlBY-81Wl"
      },
      "source": [
        "From the Transformers library, we import the necessary classes to load the model and the tokenizer.\n",
        "\n",
        "The notebook is ready to work with different Models I tested it with models from the Bloom Family and Llama-3.\n",
        "\n",
        "I recommend you to test different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pBOE-h8sbrNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd563f7-449b-4be0-d134-d438b51a16f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTTrainer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bc5eKi_efxO"
      },
      "source": [
        "## Hugging Face login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHzVpaYMfVde"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "S3LTzXUmElPk"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwAiEFifgp3-"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vziwd2UuCYGl"
      },
      "outputs": [],
      "source": [
        "#Use any model you want, if you want to do some fast test, just use the smallest one.\n",
        "\n",
        "#model_name = \"bigscience/bloomz-560m\"\n",
        "#model_name=\"bigscience/bloom-1b1\"\n",
        "#model_name = \"bigscience/bloom-7b1\"\n",
        "#target_modules = [\"query_key_value\"]\n",
        "\n",
        "model_name = \"bigscience/bloom-1b1\"\n",
        "target_modules = [\"query_key_value\"] #YOU MAY CHANGE THIS BASED ON YOUR MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVF_hKiPd1lh"
      },
      "source": [
        "To load the model, we need a configuration class that specifies how we want the quantization to be performed. We’ll achieve this with the BitesAndBytesConfig from the Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3h_ydWGf6EAd"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes>=0.43.2\n"
      ],
      "metadata": {
        "id": "0kRpLKVVJ-Wp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x47-cqrXosTc"
      },
      "source": [
        "We are specifying the use of 4-bit quantization and also enabling double quantization to reduce the precision loss.\n",
        "\n",
        "For the bnb_4bit_quant_type parameter, I've used the recommended value in the paper [QLoRA: Efficient Finetuning of Quantized LLMs.](https://arxiv.org/abs/2305.14314)\n",
        "\n",
        "Now, we can go ahead and load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "W2EZhNQ66EAd"
      },
      "outputs": [],
      "source": [
        "device_map = {\"\": 0}\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                    quantization_config=bnb_config,\n",
        "                    device_map=device_map,\n",
        "                    use_cache = False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVDiZYbee77R"
      },
      "source": [
        "Now we have the quantized version of the model in memory. Yo can try to load the unquantized version to see if it's possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "aU0awofs84q7"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtc1gbK39Hp7"
      },
      "source": [
        "## Inference with the pre-trained model.\n",
        "I'm going to do a test with the pre-trained model without fine-tuning, to see if something changes after the fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jak6FzpvFTHk"
      },
      "outputs": [],
      "source": [
        "#this function returns the outputs from the model received, and inputs.\n",
        "def get_outputs(model, inputs, max_new_tokens=100):#PLAY WITH ARGS AS YOU SEE FIT\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=1.5, #Avoid repetition.\n",
        "        early_stopping=False, #The model can stop before reach the max_length\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkFqjS459jAa"
      },
      "source": [
        "The dataset used for the fine-tuning contains prompts to be used with Large Language Models.\n",
        "\n",
        "I'm going to request the pre-trained model that acts like a motivational coach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "3BAYg7czFYeK",
        "outputId": "1983173a-b9fc-4cb7-ed52-06085561f840"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_sample' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3798017619.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtokenized_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtokenized_eval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_sample' is not defined"
          ]
        }
      ],
      "source": [
        "#Inference original model\n",
        "# input_sentences = tokenizer(\"you act like a motivation speaker\", return_tensors=\"pt\").to('cuda')\n",
        "# foundational_outputs_sentence = get_outputs(foundation_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "# print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"prompt\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_train_dataset = train_sample.map(tokenize_function, batched=True)\n",
        "tokenized_eval_dataset = eval_sample.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQUGY47p9ysI"
      },
      "source": [
        "The answer is good enough, the models used is a really well trained Model. But we will try to improve the quality with a sort fine-tuning process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL5L_DcR9ggA"
      },
      "source": [
        "## Preparing the Dataset.\n",
        "The Dataset useds is:\n",
        "\n",
        "https://huggingface.co/datasets/fka/awesome-chatgpt-prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "DyIMQ7IHFbIx",
        "outputId": "b2188ddc-7b19-40c1-8898-a2c9ea33aadd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt'],\n",
              "    num_rows: 10\n",
              "})"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = \"fka/awesome-chatgpt-prompts\"\n",
        "\n",
        "# Create the Dataset to create prompts.\n",
        "data = load_dataset(dataset)\n",
        "\n",
        "# Split the train dataset into training and evaluation samples\n",
        "train_test_split = data[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "train_sample = train_test_split[\"train\"].select(range(50)) # Take 50 samples for training\n",
        "eval_sample = train_test_split[\"test\"].select(range(10)) # Take 10 samples for evaluation\n",
        "\n",
        "# Remove the 'act' column if it exists and is not needed for tokenization\n",
        "if 'act' in train_sample.column_names:\n",
        "    train_sample = train_sample.remove_columns('act')\n",
        "if 'act' in eval_sample.column_names:\n",
        "    eval_sample = eval_sample.remove_columns('act')\n",
        "\n",
        "display(train_sample)\n",
        "display(eval_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmlZY3fk_9fm",
        "outputId": "b400665d-0b37-48c3-ee29-8d9c410661d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': ['Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.'], 'input_ids': [[186402, 1152, 1306, 660, 72560, 28857, 167625, 84544, 20165, 376, 1002, 26168, 267, 30479, 17477, 613, 267, 120755, 238776, 17, 1387, 47881, 632, 427, 14565, 29866, 664, 368, 120755, 15, 16997, 4054, 136044, 375, 4859, 12, 427, 39839, 15, 9697, 1242, 375, 13614, 12, 3804, 427, 368, 2298, 5268, 109891, 368, 17477, 15, 530, 427, 11210, 4143, 7112, 11866, 368, 11011, 1620, 36320, 17, 21265, 267, 11550, 90533, 30479, 17477, 613, 1119, 27343, 15, 11762, 368, 18348, 16231, 530, 127246, 613, 94510, 368, 25605, 55790, 17, 29901, 13842, 368, 4400, 530, 2914, 24466, 184637, 427, 22646, 267, 11285, 32391, 461, 368, 17786, 17]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ],
      "source": [
        "print(train_sample[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVPAJsrUAHiJ"
      },
      "source": [
        "## Fine-Tuning.\n",
        "The first step will be to create a LoRA configuration object where we will set the variables that specify the characteristics of the fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uCalslQFGL7K"
      },
      "outputs": [],
      "source": [
        "# TARGET_MODULES\n",
        "# https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220\n",
        "\n",
        "import peft\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16, #As bigger the R bigger the parameters to train.\n",
        "    lora_alpha=16, # a scaling factor that adjusts the magnitude of the weight matrix. It seems that as higher more weight have the new training.\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=0.05, #Helps to avoid Overfitting.\n",
        "    bias=\"none\", # this specifies if the bias parameter should be trained.\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUddynl0B1Ck"
      },
      "source": [
        "The most important parameter is **r**, it defines how many parameters will be trained. As bigger the value more parameters are trained, but it means that the model will be able to learn more complicated relations between inputs and outputs.\n",
        "\n",
        "Yo can find a list of the **target_modules** available on the [Hugging Face Documentation]( https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220)\n",
        "\n",
        "**lora_alpha**. Ad bigger the number more weight have the LoRA activations, it means that the fine-tuning process will have more impac as bigger is this value.\n",
        "\n",
        "**lora_dropout** is like the commom dropout is used to avoid overfitting.\n",
        "\n",
        "**bias** I was hesitating if use *none* or *lora_only*. For text classification the most common value is none, and for chat or question answering, *all* or *lora_only*.\n",
        "\n",
        "**task_type**. Indicates the task the model is beign trained for. In this case, text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HArPQ_lvGUkY"
      },
      "outputs": [],
      "source": [
        "#Create a directory to contain the Model\n",
        "import os\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWalmqWm4STo"
      },
      "source": [
        "In the TrainingArgs we inform the number of epochs we want to train, the output directory and the learning_rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ND0aJ-t6ARqD"
      },
      "outputs": [],
      "source": [
        "#Creating the TrainingArgs\n",
        "import transformers\n",
        "from transformers import TrainingArguments # , Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_directory,\n",
        "    auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
        "    learning_rate= 2e-4, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgxsV-iy_J_o"
      },
      "source": [
        "Now we can train the model.\n",
        "To train the model we need:\n",
        "\n",
        "\n",
        "*   The Model.\n",
        "*   The training_args\n",
        "* The Dataset\n",
        "* The result of DataCollator, the Dataset ready to be procesed in blocks.\n",
        "* The LoRA config.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "z5NYHqBnGZyF"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "trainer = SFTTrainer(\n",
        "    model=foundation_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    peft_config = lora_config,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "kEKiFdpDGgOx"
      },
      "outputs": [],
      "source": [
        "#Save the model.\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "s6Hmy87-HViP"
      },
      "outputs": [],
      "source": [
        "#Save the model.\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "glDE0Z4FOSe0"
      },
      "outputs": [],
      "source": [
        "#In case you are having memory problems uncomment this lines to free some memory\n",
        "# import gc\n",
        "# import torch\n",
        "# del foundation_model\n",
        "# del trainer\n",
        "# del train_sample\n",
        "# torch.cuda.empty_cache()\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOAqEg0mSjHW"
      },
      "source": [
        "## Inference with the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "dX5d8xMCSC6y"
      },
      "outputs": [],
      "source": [
        "#import peft\n",
        "from peft import AutoPeftModelForCausalLM, PeftConfig\n",
        "#import os\n",
        "\n",
        "device_map = {\"\": 0}\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")\n",
        "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "CA-E-io_Dfe1"
      },
      "outputs": [],
      "source": [
        "bnb_config2 = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_TAjrSWSe14q"
      },
      "outputs": [],
      "source": [
        "#Load the Model.\n",
        "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "                                        peft_model_path,\n",
        "                                        #torch_dtype=torch.bfloat16,\n",
        "                                        is_trainable=False,\n",
        "                                        #load_in_4bit=True,\n",
        "                                        quantization_config=bnb_config2,\n",
        "                                        device_map = 'cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK--YFPR6OxH"
      },
      "source": [
        "## Inference the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_27uvJudf03",
        "outputId": "00b0dadd-261e-4b3c-eb2e-01a423ef3e32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"I want you to act as a motivational coach.  I don't know if this is the right place, but I'm trying.\\nI'm not sure how much of an influence my parents have on me. My dad was very supportive and encouraging in his life; he never gave up or stopped working for anything that\"]\n"
          ]
        }
      ],
      "source": [
        "input_sentences = tokenizer(\"I want you to act as a motivational coach. \", return_tensors=\"pt\").to('cuda')\n",
        "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCV9JOBG6Ug8"
      },
      "source": [
        "The result is really good. Let's compare the answer of the pre-trained model with the fine-tuned one:\n",
        "\n",
        "* **Pretrained Model**: 'I want you to act as a motivational coach. \\xa0You are going on an adventure with me, and I need your help.\\nWe will be traveling through the land of “What If.” \\xa0 This is not some place that exists in reality; it’s more like one those places we see when watching'\n",
        "\n",
        "* **Fine-Tuned Model**: 'I want you to act as a motivational coach.  I will provide some information about an individual or group of people who need motivation, and your role is help them find the inspiration they require in order achieve their goals successfully! You can use techniques such as positive reinforcement, visualization exercises etc., depending on what'\n",
        "\n",
        "As you can see, the result is really similar to the samples contained in the dataset used to fine-tune the model. And we only trained the model for some epochs and with a really small number of rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq0ZeeeyepF-"
      },
      "source": [
        " - Complete the prompts similar to what we did in class.\n",
        "     - Try a few versions if you have time\n",
        "     - Be creative\n",
        " - Write a one page report summarizing your findings.\n",
        "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
        " - What did you learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learnings:\n",
        "I learned that QLoRA is a smart and resource-efficient way to fine-tune massive language models by combining quantization with adapter-based training. It lets anyone customize large LLMs — even with limited hardware — while keeping performance high."
      ],
      "metadata": {
        "id": "d_CVDal4P5e6"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}